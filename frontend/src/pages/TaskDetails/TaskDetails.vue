<script lang="ts" setup>
import { ref, onMounted, onUnmounted } from 'vue';
import { ElMessage } from 'element-plus';
import axios from 'axios';
import { useRoute } from 'vue-router';
import { useStore } from 'vuex';

const route = useRoute();
const store = useStore();
const taskId = route.params.task_id as string; // è·å–ä»»åŠ¡ID
const userId = Number(store.state.user.id); // è·å–ç”¨æˆ·ID
const token = store.state.user.token; // è·å–token
const phase = ref(1);
const isRecognizing = ref(false);  // è®°å½•æ˜¯å¦æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«
const isVoiceInteraction = ref(false);
const aiResponse = ref<{ text: string, type: string, content: string[] }[]>([]);
const aiImages = ref<{ [key: string]: string[] }>({});
const selectedResponse = ref<string | null>(null);
const isTaskAreaFocused = ref(false);
const isAIResponseFocused = ref(false);
const recognizedText = ref(''); // å­˜å‚¨è¯†åˆ«çš„æ–‡æœ¬
const displayText = ref(''); // åŠ¨æ€æ˜¾ç¤ºçš„æ–‡æœ¬
const task = ref<any>(null); // å­˜å‚¨ä»»åŠ¡å¯¹è±¡

//ä¾æ®è·¯ç”±ä¼ è¿‡æ¥çš„taskIdæ¥æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡
const fetchTask = async (id: string) => {
  try {
    const response = await axios.get(`http://localhost:3001/api/tasks/${id}`, {
      headers: {              // åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ  Authorization
        Authorization: `Bearer ${token}`,
      },
    });
    task.value = response.data.task; // å‡è®¾åç«¯è¿”å›ä»»åŠ¡å¯¹è±¡
    console.log(task.value);
  } catch (error) {
    console.error('è·å–ä»»åŠ¡å¤±è´¥:', error);
  }
};

function setPhase(phaseNumber: number) {
      phase.value = phaseNumber;
    }

// æ·»åŠ å¯¹ SpeechRecognition çš„ç±»å‹å®šä¹‰
declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}

let audioContext: AudioContext | null = null;
let audioStream: MediaStream | null = null;
let socket: WebSocket | null = null;
let isRecording = false;
const SAMPLE_RATE = 16000;
const BUFFER_SIZE = 4096;
const success_message = ref('');

function updateRecognitionResult(text: string) {
  recognizedText.value = text;
  displayText.value = text; // åŠ¨æ€æ›´æ–°æ˜¾ç¤ºæ–‡æœ¬
}

function connectWebSocket() {
  const wsUrl = `ws://localhost:3001/ws?token=${token}`;
  socket = new WebSocket(wsUrl);

  socket.onopen = () => {
    console.log('WebSocketè¿æ¥æˆåŠŸ');
    isRecognizing.value = true;  // è¿æ¥æˆåŠŸåå¼€å§‹è¯­éŸ³è¯†åˆ«
  };
  

  socket.onclose = (event) => {
    console.log('WebSocketè¿æ¥å…³é—­', event);
    isRecognizing.value = false;  // WebSocketå…³é—­ååœæ­¢è¯­éŸ³è¯†åˆ«
    if (isRecording) {
      setTimeout(connectWebSocket, 1000);
    }
  };

  socket.onmessage = (event) => {
    try {
      const data = JSON.parse(event.data);
      if (data.result) {
        updateRecognitionResult(data.result);
      } else if (data.text) {
        updateRecognitionResult(data.text);
      } else if (data.error) {
        updateRecognitionResult('é”™è¯¯: ' + data.error);
      }
    } catch (e) {
      console.error('è§£ææ¶ˆæ¯å¤±è´¥:', e);
    }
  };

  socket.onerror = (error) => {
    console.error('WebSocketé”™è¯¯:', error);
    isRecognizing.value = false
  };
}

async function initAudio() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        channelCount: 1,
        sampleRate: SAMPLE_RATE,
        sampleSize: 16,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
    });

    audioStream = stream;
    audioContext = new (window.AudioContext)({
      sampleRate: SAMPLE_RATE,
    });

    const source = audioContext.createMediaStreamSource(stream);
    const processor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1);

    source.connect(processor);
    processor.connect(audioContext.destination);

    processor.onaudioprocess = function (e) {
      if (!isRecording || !socket || socket.readyState !== WebSocket.OPEN) return;

      const inputData = e.inputBuffer.getChannelData(0);
      const pcmData = new Int16Array(inputData.length);

      for (let i = 0; i < inputData.length; i++) {
        const gain = 1.5;
        let sample = Math.max(-1, Math.min(1, inputData[i] * gain));
        pcmData[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
      }

      socket.send(pcmData.buffer);
    };

    return true;
  } catch (error) {
    console.error('åˆå§‹åŒ–éŸ³é¢‘å¤±è´¥:', error);
    return false;
  }
}

async function startVoiceInteraction() {
  isVoiceInteraction.value = true;
  aiResponse.value = []; // æ¸…ç©ºAIå›å¤
  selectedResponse.value = null;
  isTaskAreaFocused.value = true;
  isAIResponseFocused.value = false;
  recognizedText.value = '';
  displayText.value = '';

  if (!socket || socket.readyState !== WebSocket.OPEN) {
    connectWebSocket();
  }

  if (!audioContext) {
    const initialized = await initAudio();
    
    if (!initialized) {
      alert('æ— æ³•å¯åŠ¨éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™');
      return;
    }
  }
  ElMessage.success('å¼€å§‹è¯­éŸ³è¯†åˆ«');
  isRecording = true;
}

function stopVoiceInteraction() {
  if (!isRecording) return;
  isVoiceInteraction.value = false;
  isTaskAreaFocused.value = false;
  isAIResponseFocused.value = true;
  isRecording = false;

  if (audioStream) {
    audioStream.getTracks().forEach((track) => track.stop());
    audioStream = null;
  }

  if (audioContext) {
    audioContext.close();
    audioContext = null;
  }

  if (socket && socket.readyState === WebSocket.OPEN) {
    socket.close();
  }

  getAiResponse(recognizedText.value);
}

function getAiResponse(text: string) {
  const requestBody = {
    studentId: userId,
    response: text,
    phase: phase.value,
    taskId: taskId, // Use the actual taskId
  };

  axios
    .post('http://localhost:3001/api/ai/questions', requestBody,{
    headers: {              // åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ  Authorization
        Authorization: `Bearer ${token}`,
      },}
    )
    .then((response) => {
      // Handle the response from the backend
      console.log(response.data);
      const questions = response.data;
      if (Array.isArray(questions)) {
        aiResponse.value = questions.map((item: { question: string; hints: string[] }) => ({
          text: item.question,
          type: 'image',
          content: item.hints.map((hint) => `data:image/png;base64,${hint}`),
        }));
      } else {
        console.error('Unexpected questions format:', questions);
      }
    })
    .catch((error) => {
      console.error('Error fetching AI response:', error);
    });
}

// AIå›å¤ç‚¹å‡»é€»è¾‘
function handleAIResponseClick(response: { text: string; type: string; content: string[] }) {
  if (selectedResponse.value === response.text) {
    selectedResponse.value = null;
    if (response.type === 'image') {
      aiImages.value[response.text] = [];
    }
  } else {
    selectedResponse.value = response.text;
    if (response.type === 'image') {
      aiImages.value[response.text] = response.content;
    }
  }
  isAIResponseFocused.value = true;
  isTaskAreaFocused.value = false;
}

onMounted(() => {
  fetchTask(taskId);
});

onUnmounted(() => {
  stopVoiceInteraction();
});
</script>

<template>
  <div class="container">
    <div
      class="task-area"
      :class="{ 'full-screen': isVoiceInteraction, 'blurred': aiResponse.length > 0 && !isTaskAreaFocused, 'focused': isTaskAreaFocused }"
    >
    <!-- æŒ‰é’®é€‰æ‹©æ¡† -->
    <div class="phase-selector">
      <button
        :class="{ active: phase === 1 }"
        @click="setPhase(1)"
      >
        å®Œæ•´æ€§è€ƒéªŒ
      </button>
      <button
        :class="{ active: phase === 2 }"
        @click="setPhase(2)"
      >
        é€»è¾‘æ€§è€ƒéªŒ
      </button>
      <button
        :class="{ active: phase === 3 }"
        @click="setPhase(3)"
      >
        æƒ…æ„Ÿæ€§è€ƒéªŒ
      </button>
    </div>
    <!-- <div class="phase-selector">
          <label for="phase">é€‰æ‹©è€ƒéªŒç±»å‹ï¼š</label>
          <select id="phase" v-model="phase">
            <option value="1">å®Œæ•´æ€§è€ƒéªŒ</option>
            <option value="2">é€»è¾‘æ€§è€ƒéªŒ</option>
           <option value="3">æƒ…æ„Ÿæ€§è€ƒéªŒ</option>
         </select>
       </div> -->
      <!-- ä»»åŠ¡åŒºåŸŸå†…å®¹ -->
      <div v-if="task"   class="task-images">
        
        <img v-for="image in task.images" :key="image.imageOrder" :src="image.imageUrl"  alt="ä»»åŠ¡å›¾ç‰‡" />
      </div>
      <div v-if="displayText" class="animated-text">{{ displayText }}</div>
      
    </div>
    <!-- é€‰æ‹©è€ƒéªŒç±»å‹ -->
    
    <!-- AIå›å¤ -->
    <div v-if="aiResponse.length > 0" class="ai-response">
      <div
        v-for="response in aiResponse"
        :key="response.text"
        @click="handleAIResponseClick(response)"
      >
        <div class="message">
          <img class="avatar" src="../../assets/image/robot.png" alt="Robot Avatar" />
          <div class="response">
            <div>{{ response.text }}</div>
            <div v-if="response.type === 'text' && selectedResponse === response.text">
              {{ response.content }}
            </div>
            <div v-if="response.type === 'image' && selectedResponse === response.text">
              <span
                v-for="image in aiImages[response.text]"
                :key="image"
                >
                <img :src="image" :alt="'Image ' + image" class="ai-image"/>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="voice-button-container" :class="{ 'bottom-right': aiResponse.length > 0 }">
  <div
    class="voice-button"
    :class="{ active: isVoiceInteraction, recognizing: isRecognizing }"
    @click="isVoiceInteraction ? stopVoiceInteraction() : startVoiceInteraction()"
  >
    <span v-if="isRecognizing">ğŸ¤</span> <!-- æ­£åœ¨è¯†åˆ«æ—¶æ˜¾ç¤ºéº¦å…‹é£å›¾æ ‡ -->
    <span v-else>ğŸ¤</span>  <!-- é»˜è®¤æ˜¾ç¤ºéº¦å…‹é£å›¾æ ‡ -->
  </div>
</div>
  </div>
</template>

<style lang="less" scoped>
html, body {
  margin: 0;
  padding: 0;
  height: 100%;
  overflow: hidden;
}

.container {
  height: 80vh;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  background-color: #f9f7f7;
}

.task-area {
  flex: 1;
  width: 100%;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  background-color: rgb(151, 218, 247);
  position: relative;
  margin: 0; /* ç§»é™¤å¤–è¾¹è· */
  padding: 0; /* ç§»é™¤å†…è¾¹è· */
}
.phase-selector {
  display: flex;
  justify-content: space-around;
  margin-bottom: 20px;
}

.phase-selector button {
  padding: 10px 20px;
  border: none;
  border-radius: 5px;
  background-color: #f0f0f0;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.phase-selector button.active {
  background-color: #007bff;
  color: white;
}
/* å–æ¶ˆæ‚¬åœæ—¶å˜è‰²çš„æ•ˆæœ */
.phase-selector button:hover {
  background-color: #f0f0f0;
}

.phase-selector button.active:hover {
  background-color: #007bff; /* ä¿æŒé€‰ä¸­çš„æŒ‰é’®é¢œè‰²ä¸å˜ */
}
.task-images {
  width: 100%;
  height: 100%;
  display: flex;
  justify-content: center; /* å±…ä¸­å›¾ç‰‡ */
  align-items: center; /* å‚ç›´å±…ä¸­å›¾ç‰‡ */
  gap: 10px; /* å›¾ç‰‡é—´è· */
}

.task-images img {
    width:20vw;
    height:40vh;
  object-fit: cover; /* å›¾ç‰‡å¡«å…… */
  margin: 0 5px;
}

.animated-text {
  font-size: 2.5rem;
  font-weight: bold;
  color: #21b371;
  animation: fade-in 1s ease-in-out;
  margin-top: 20px;
}

@keyframes fade-in {
  from {
    opacity: 0;
    transform: scale(0.8);
  }
  to {
    opacity: 1;
    transform: scale(1);
  }
}

.ai-response {
  width: 100%;
  background:  rgb(151, 218, 247);
}

.message {
  display: flex;
  align-items: flex-start;
  margin-bottom: 16px;
}

.avatar {
  width: 50px;
  height: 50px;
  border-radius: 50%;
  margin-right: 12px;
}

.response {
  background: rgb(107, 182, 236);
  padding: 10px 15px;
  border-radius: 18px;
  text-align: left;

  
  max-width: 60vw;
}

.voice-button-container {
  position: fixed;
  bottom: 50px;
  display: flex;
  justify-content: center;
  width: auto;
  &.bottom-right {
    right: 50px;
    justify-content: flex-end;
  }
}

.voice-button {
  width: 80px;
  height: 80px;
  background-color: #42b883;
  border-radius: 50%;
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
  display: flex;
  justify-content: center;
  align-items: center;
  font-size: 2rem;
  color: white;
  cursor: pointer;
  transition: transform 0.3s, background-color 0.3s;
}

.voice-button.active {
  background-color: #2fc47a;
  transform: scale(1.2);
}
.ai-image {
  
  display: inline-flex;
  flex-wrap: nowrap; /* Ensure images are displayed in a single row */
  margin-left: 10px;
  height: 100px; /* Set a fixed height for images */
}
// .voice-button.recognizing {
//   background-color: #42b883; /* è¯†åˆ«ä¸­æ—¶çš„èƒŒæ™¯è‰² */
// }

</style>
